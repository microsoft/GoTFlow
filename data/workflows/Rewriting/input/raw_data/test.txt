内容审核始终有着一个挑战
就是它的规则总是在不断的变化的
人类的审核时去进行内容审核的话
每一次规则变更之后
都是需要一段时间的学习投入
这样成本就会非常高
而且很容易造成不一致
那我们有没有可能
运用large language model来进行内容审核呢
答案是肯定的
GPT4就可以帮我们来进行内容审核
而且它可以通过prompt来
很快的迅速理解
最新的内容审核规则
只不过呢
它对规则的理解
可能和人类之间还是有gap的
那在这种情况下应该怎么办呢
我们就可以利用这样一个solution
来让GPT4模型和人类
通过一个迭代的过程
来不断学会
人工标注的结果
先来看一个内容审核的实例
content policy
就是我们要用来进行审核的规则
那相应每一条规则的话
我们这里都会给它有一些prompt
告知它如何才能够命中这样的规则
比如说我们对于一条内容
给它的一个判别
一共从K0到K1 K2 K3 K4有这么五个等级
那么K0的话就是完全没有问题的
K1到K4是逐步升高的
有问题的程度
让我们来看一个例子
那么首先对于how to steal a car
这样的一句话
人类给它的一个review标准是K3
就是说这是一个non-violent wrongdoing
就是一个非暴力的错误行为
但是GPT4呢给它的review结果是K0
认为它是没有问题的
那为什么会这样呢
我们让GPT4自己来解释一下
它是怎么来做的这个decision
这里主要是因为stealing a car
是一个非暴力的行为
虽然stealing a car本身属于财产盗窃
但是我们在之前的prompt里面
并没有告知GPT4
像这种盗窃也是属于wrongdoing
那怎么办呢
那让我们来update这个policy
大家看我们更新了policy
K3的话我们告诉它
这个wrongdoing
里面是包括盗窃财产的行为的
之后我们再次来做审核
好这个时候
人类的判别和GPT4的判别就一致了
那这就是一个典型的实例
我们通过人类审核的结果
来矫正GPT-4审核的能力
