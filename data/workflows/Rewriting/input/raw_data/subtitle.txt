OpeAI 发布的Sora模型
再一次引起了业界的轩然大波
大家都惊叹于它现在的效果
Sora是怎么做到这一步的呢
虽然没有论文
但 OpenAI 是发布了技术报告的
在这篇报告里面
基本上讲清楚了它的整个技术路径
当然它自己的陈述还是比较high level的
没有涉及到具体的技术点
不过在这篇技术报告的最后
释放出了全部的参考文献
一共是32篇
就是现在的这些
从这32篇文献里面
其实我们可以看到
整体训练模型的方法论
我总结了一下这32篇论文
首先做了这样一张思维导图
我们可以把这些论文分成几个大的
不同的块
而每块里面呢
还可以再分一些小块
那可见这些论文它是有结构的
其实整个的这些论文
所有它们加起来
既有一个 end2end 的方法论
然后又包括这个模型
具体的操作层面的内容
这个我们稍后讲
我们先来看一下
这些论文的出处和来源
它一共32篇论文
其中12篇直接就是 arXiv.org 上面的论文
也就是说
它们的作者其实都没有去投这些会议啊
期刊啊只是把它publish到了 arXiv 上面
这是很大的一块
超过1/3了
那么其他的呢
就是还是几个顶会为主
像NIPS啊
ICML，CVPR 和 ICCV

这几个来源呢
其实总共加起来就超过了50%的比重啊
可见 Sora 技术的最主要的来源
还是这些传统意义上的顶会
那么除此之外
当然还有ECCV
还有 OpenAI 自己
那么从年代上面呢
大家可以看哈
我们右边列的是年代
那么最早的一篇也是2013年
也就说所有的内容都来自于近十年
那因为它是在2023年完成的
model training的过程
所以他们开始做的时候
其实是更早的时候
那个时候
肯定是要借助2022年的论文
所以2022年的论文的比重
在整个的每个年代当中是最多的
我们可以看
21，22，23这三年的加起来
其实已经超过一半了
那么最主要的
还是来依靠新的论文来做的
所有的这些事情
我们刚才那个思维导图
是从图的结构上面
给大家一个整体的
直观感受
那么现在我们就来看看
Sora 参考的论文实际上是怎么来划分的
那首先论
有一大块是在讲
怎么对视频数据进行生成式建模
这个实际上是一个完整的方法论了
所谓视频生成
我要把这个视频数据给生成出来
那我要先对它有一个modeling的过程
我才可能是说我是对它进行判别
还是对它进行生成
所以视频生成
整体上是属于视频建模的一个子集
我们要做视频生成这件事
情肯定啊
它的前序的工作
那就是先要去了解
完整的视频建模的技术发展路径
那么我们可以看这一一个大块下面
就整个这一个大块下面又分几个小块
就包括最早期的用RNN进行建模
然后用GAN
然后是自回归的Transformer
然后最后才到 diffusion model
那么除了视频数据的处理之外
其实Sora还借鉴了
大型语言模型的很多的技术点
所以呢还有一部分论文
是大型语言模型这方面的
然后
从第三个到第五个
这三块加起来
可以说就是我们训练一个模型
最基础的几个部分
一个是数据处理
一个是具体的学习方法
还有一个就是网络架构
它数据处理
其实最主要的一块在于patch
Sora的话
把视频图像
以及这个二维的普通的图像
都是把它切割成了一个一个的patch
来做的处理
那这样的话
它可以用patch来模拟文本当中的TOKEN
所以它就可以用Transformer
那在这一块
视频处理
在数据处理的这一块
它关于patch相关的论文有好几篇
那之后的话呢
当然就是对视觉数据
我们可以想见哈
它一定要比文本数据的信息量
要大得多
那么我们要处理它的话
那一个方面呢
是要从里面提取真正的有效信息
另外一个方面呢
也是为了加快
训练速度
一定会对这类数据
有一个压缩和建模的过程
所以还有一些论文
是相关于这个部分的
那么它的学习方法呢
那当然就是扩散模型
这个已经非常简单了
我们说扩散模型的时候
其实很主要的讲的
是一个具体的训练方式
再之后是它的网络架构
那diffusion Transformer
这是就没什么可说的了
还有一些相关的图像生成
transformer的论文
那么最后还有一块呢
实际上他讲到的是
比如说像这个recaptioning啊
用在DallE3里面的这些
具体的提高图像质量的方法啊
等等它还有一些是借鉴了一些技术
去提升它的
这个生成出来数据的质量
那我们现在分别来看一下哈
我们先看看
就是视频数据的生成式建模这一块
那么最早的话
用的就是RNN啊
RNN的这几篇论文呢
大家可以看它最早一篇是2015
然后2017，2018啊
Transformer 架构是2017年
Attention is all you need
才发表出来
所以 2015的时候
还没有Transformer
对于视频来说
它又是一个序列数据
所以那个时候
肯定还是要用这种
我们比较习惯性的
处理序列数据的模型
来处理它
那就是这种广义上的这个RNN
第一篇
实际上用的是LSTM
使用多层的LSTM网络
来学习视频序列的表示
到了2017年这篇论文
已经开始用循环神经网络
来进行这种时空连续预测了
那这个时候
我们就可以说是向着生成
又走上走出了一步啊
然后之后呢
这个很著名的这个世界模型
它构建了一个强化学习的环境
然后在这个里面呢
让生成式的神经网络去进行学习
这样的话可以进行无监督的训练
还可以在这种无监督的情况之下
去学习环境的
压缩的空间和时间的表示
这个是我们在RNN里面啊
可以看到的一个技术的发展
之后
就是GAN
最初的话
第一篇论文是利用大量未标记的视频
来学习这种场景的一个动态模型
实现了场景的
前景和背景的分离
那然后第二篇论文呢
实际上在前景的情况下
要是区分内容和运动
在保持这个内容部分固定的同时
又可以实现运动部分
大家我们带入到一个现实的视频里
就可以理解了吧
比如说这个视频是
背景是一片一个操场
前景是一个人在跑步
这个操场大致的这个景象
没有什么变化的
但是人就会变得很多
同样是这个人
他在这一秒哎
可能他左胳膊在前哎
下一秒变成右胳膊在前
他这个人还是这个人
内容没有变
服装也没有变
他的长相也没有变
身体的这个肌肉也没有变
但是呢他的姿态变了很多
也就是说他的运动改变了很多
那如果我们不能够区分这些的话
那只是强行的去生成一个个Pixel
那肯定是很难搞的对吧
但如果能区分的话
那就肯定是对整体上的
这个视频生成啊
就已经提供了非常强的一种工具
那么在后面呢
到了2019年的时候呢
实际上是实现了更长
更高分辨率的视频生成
那这个视频生成的时间
也是很关键的一个点
我不知道大家有没有用过 runway 啊
Pika 呀
这样的视频生成的在线服务
就是至少它们开放出来的接口的话
能生成的也就是1-3秒的这样的
这么这么样的一个长度的视频
一一般都是1秒多一点
2秒差不多是这么一个长度
对于视频来说
这可能是确实是有点短
生成更长时间更高质量的视频
一直是视频生成领域的一个追求
那么最后这篇论文
实际上呢
它是可以准确再现物体运动
还有这个摄像机视角的变化
如果我们人类拍的视频的话
你的这个机位
这个取景的
你的这个镜头本身是特别重要的
你是广角
你还是特写对吧
你还是一个长镜头
还是什么
都非常非常的重要
那么我们在生成场景当中
它可以来生成这样的
针对这种摄像机视角的变化的
新的内容
就是也是技术上的一个突破了
自回归的Transformer
来生成这种视频数据啊
那这里面呢
就两篇论文
第一篇呢
基于似然的生成式建模
把它扩展到了视频当中
实际上是一个技术借鉴的过程
那么还有一点呢
就是它用VQ-VAE
来处理这种视频的这个数据
来给数据做编码
那么现在基本上你处理视频
就是不管你处理什么视频
都会拿这个VQ-VAE来怼一下
这个也的确是现在被大家采用的
非常广泛的一种办法
还有一个呢
就是微软啊
微软MSRA
亚洲研究院提出的
女娲模型
它实际上是一个多模态的
这样的一个模型啊
而且呢很关键的一点呢
它还实现了零样本
也就是Zero Shot 的能力
下面
就是在用扩散模型来进行视频生成
这里面的三篇文章
最早的一篇也是2022年的
然后两篇是23年的
可见这是一个很新的一个技术
之前我们大家都知道stable diffusion嘛
那么著名了已经
它是用这个扩散模型来生成图像
那么这个第一篇文章呢
其实就是将扩散模型
应用到了视频生成上面啊
我们应该说是技术的一个迁移
第二个就是第二篇论文啊
通过这种压缩过的
低维潜在空间的数据表示
来训练扩散模型的这样一种方法
来实现高质量的图像合成
那这个方法其实后面
它会被反复的给提到啊
那么第三篇呢
实际上是通过扩散模型
实现了一个真实感视频的一个生成
那这个很重要对吧
那如果你生成出来的都是那种啊
就是漫画风的
或者说看起来就不是真人的那种
那总是
会有很大的局限
但如果直接就生成出来是真人的状态
那肯定就
可用的场景那就是要广泛的多了
那么对于大型语言模型的借鉴呢
这里其实就提到了两篇论文
第一个就不用说了嘛
Attension is so you need
这个Transformer的神
那么还有一篇呢
就是训练GPT3模型的对应的那篇论文
然后
我们就进入到
数据处理的这个部分了
那从现在开始
后面呢实际上就是已经到了他
具体真的去
训练模型
要用到的那些具体的技术
就是最起码的一点
就是数据的处理对吧
那数据的处理呢
首先就是啊
第一篇论文实际上是2020年的
实际上就是用patch
这个方法来处理图像数据嘛
它自己做的是识别
但是没关系
就是在这个里面
还是用了这个patch
来处理具体的图像数据
这个才是关键
就是通过把这个图像
切成一个个的小方块
每个小方块
你可以类比成一篇文章里的一个字，一个TOKEN
这样它就可以用Transformer来处理它了
第二篇呢
开始处理视频数据
视频数据
相对于图像数据来说
它还有一个时间轴
实际上它是要提取时空标记的
那它就不仅仅
是一个二维的
这么的一个小方块
它实际上就变成了一个三维的
那么一个小的立方体
或者一个长方体
就变成了这样一个东西
你可以这样想象一下
然后这里面的第三篇
标号17的这篇
这个是何恺铭的大作了啊
他这里面讲的实际上就是把一个图像
我把它切分成若干的patch之后
然后我去遮挡住其中的一部分patch
然后我再让这个模型
去预测被遮挡住的那些patch
以此啊
来完成一个这种自编码的生成图像
生成模型的这样的一个学习
基于patch去生成对吧
那么然后最后这篇呢
就标号18的这个
实际上它是把分辨率作为一个参数
引入了其中
那它在处理数据的时候
它不再像之前那样是强行的
把我所有的这些样本
都切分成一个固定的长宽
不再是那么做了
而是说
我原本是什么分辨率就是什么分辨率
那我把这个原图
以及它的分辨率都作为输入
输入给这个模型
然后这个模型
它同时处理了具体的内容
和相关的分辨率之后
也相应的获得了可以根据用户的要求
去生成不同分辨率的这个数据的
这种灵活性
除了patch之外
还有一个就是降维啊
其实就是怎么把这个视频数据
进行一个尽量无损的压缩
我们简单来说就是这样
这个就是两篇文章啊
这个就是涉及到比较细节的内容
就具体怎么去压缩这个东西
当然最主要的一点
就是为了利用有限的计算资源
否则的话
你维度太高的话
这个计算量太大了对吧
这部分第一篇
是通过在这个模型里面
引入
交叉注意力层
第二篇
它引入了一种随机的变分推断的
这样的一种学习算法啊
这样的话使得训练
可以扩展到更大的数据集上面
然后我们就来到了
学习方式的这一块
那就是扩散模型啊
大家知道扩散模型
我原来专门有过一个视频
讲扩散模型的
当时讲的是一个图像
生成的一个扩散模
它实际上就是我把一个图像
一点点给它加噪
加噪一直加到纯噪声
然后之后把这个作为训练数据
然后让它反过来
从纯噪声开始一步一步的predict
先predict的稍微有一点轮廓的
然后轮廓更清晰一点的
最后一步步到一个完整的
清晰的一个图像
这么一个过程
它本身就是借鉴了物理学当中的一个
非平衡热力学里面的扩散的这个概念
我们看第一篇论文
它实际上是2015年的论文
它就是借鉴了物理学当中的这个
扩散概念
然后
就是开发了一个迭代前进的
通过扩散过程
然后系统的缓慢的
把数据当中被破坏的结构
整合起来的
这样的一个算法
再之后是使用扩散模型
用它来生成比较高质量的图像
这个是2020年的论文了
然后再之后呢
从数学上证明了
它是可以保持高样本质量
这样的一个生成的
再之后
证明了
扩散模型可以在图像样本质量上
超越当前所有的生成模型
扩散模型
其实它
为什么现在用的这么多
是因为它很扎实哈
它一步一步
是从数学原理上推导出来的
所以
它现在可以这样的大放异彩
这也是原因之一吧
那么最后的话
实际上是因为扩散模型它是很复杂的
无论它的理论还是它的实践
都非常复杂
那么最后这篇论文呢
它实际上是提出了一个设计空间
就给了我们一个更好的
这样的一个训练方法
以及调优他的一个方法
到了网络架构这块
diffusion Transformer就是一篇论文
这个也没什么可说的
它提出了是在这个扩散模型里面
用Transformer来替代原本的这个U-Net架构
我们知道像stable diffusion的话
它就用的是U-Net架构
训练了扩散模型
那这边
是用Transformer去替代了U-Net
这篇论文的作者其中之一
这个谢赛宁啊
前一段时间
网上也有一段关于他的这个谣言
他还自己出来辟谣了
这里有点八卦
大家有兴趣可以看一下
那这个图像生成Transformer
实际上就是
从2020年之后
20年21年和22年的三篇论文
也都是比较新的
第一篇主要讲的是一个是
就是训练一个序列Transformer
然后来自回归的
预测像素
这是第一篇啊
第二篇论文
它把文本信息和图像的信息
整合在一起了
作为单一的数据流进行建模
这个其实对于我们图像生成
是非常重要的
因为我们图像生成吗
就是大家有一个非常重要的应用
就是通过文本的prompt提示
来要求它生成
那这个时候呢
它必须要是能够理解
文本的输入
那么这篇论文
实际上是
给我们提供了这样的一个指导
那么第三篇呢
就是把图像啊
把那个文本到图像的生成
直接就看成了一个序列到序列
的建模的问题
这个实际上是基于第二篇
就更近了一步
我就纯粹的让你输入文本
来生成图像啊
就是在类比一个翻译模型
翻译模型就是说
比如说你输入英文
输出中文
输入的是一个语言的TOKEN
输出的是另外一个语言的TOKEN
那它这里其实就是说
我输入的都是文本的TOKEN
但输出的可能就是若干的patch
等等的这些
有了这样的一个技术的话
我们就可以基于文本prompt
来生成视频
最后
有几篇
实际上是提高图像质量
那有一块就是这个recaptioning
在DallE3里面也是OpenAI自己
特别喜欢讲的一个技术点
就是简单来说呢
你的那些训练模型的图像
可能没有足够好的
说明自己内容的文本
并没有说
sample 图像
有一个很详细的说明
那这个时候怎么办
它先训练一个模型
去给这些训练样本图像
生成很多文本的说明
然后再用生成出来这个文本
和图像一起去进行训练
然后发现效果很好
然后第31这篇呢
实际上是证明的是显示生成的图像表示
可以在保持图像原本的真实性
还有说明相似性的前提之下
来提高图像的多样性
最后一篇
实际上就是一个用来生成逼真图像的
这样一个方法
通过迭代去噪随机微分方程
来生成逼真的图像
这个又是一个比较细节的一个东西了
这就是Sora的全部的32篇参考论文
大家如果想要下载这些论文的PDF
其实你可以完全一篇一篇自己去找
这些都没有问题
当然我们也为大家都下载下来了
那就放在这个位置
就是这个URL里面
大家可以自己去下载
视频生成进入了最新的一个阶段