首先，我要说的是，这篇文章并不是对LLMs的回顾。很明显，2023年对于人工智能来说是一个特殊的年份，再重申这一点似乎没有什么意义。相反，这篇文章旨在作为一名个体程序员的见证。自从ChatGPT问世以来，以及后来通过使用本地运行的LLMs，我已经广泛地利用了这项新技术。目标是加速我的编码能力，但这并不是唯一的目的。还有一个意图是不把精力浪费在编程中不值得付出努力的方面。无数小时花在寻找关于奇特、智力上无趣的方面的文档；努力学习一个过于复杂的API，往往没有充分的理由；编写立即可用的程序，但几小时后就会丢弃。这些都是我不想做的事情，尤其是现在，谷歌已经变成了一个充满垃圾信息的海洋，要在其中寻找一些有用的东西。
然而，根据我过去几个月的经验，对于系统编程来说，如果你已经是一名有经验的程序员，那么LLM几乎永远无法提供令人满意的解决方案。让我给你展示另一个真实的例子。我目前的项目，ggufflib，涉及编写一个读取和写入GGUF格式文件的库，这是llama.cpp加载量化模型所使用的格式。最初，为了理解量化编码是如何工作的（出于速度原因，每个量化位以一种奇特的方式存储），我尝试使用ChatGPT，但后来我决定逆向工程llama.cpp的代码：这样做要快得多。一个能够为系统程序员提供适当帮助的LLM，如果看到数据编码的“结构”声明和解码函数，应该能够重建数据格式文档。llama.cpp的函数足够小，可以完全放入GPT4的上下文中，但输出却完全无用。在这些情况下，事情还是像过去一样进行：纸和笔，阅读代码，看解码器提取的位在哪里被记录。
让我更好地解释一下上面的用例，以便您在需要时可以自己尝试。我们从llama.cpp实现中得到了这个结构。

// 6位量化
// 权重表示为 x = a * q
// 16个16元素的块
// 每个权重有效位为6.5625位
typedef struct {
    uint8_t ql[QK_K/2];      // quants, lower 4 bits
    uint8_t qh[QK_K/4];      // quants, upper 2 bits
    int8_t  scales[QK_K/16]; // scales, quantized with 8 bits
    ggml_fp16_t d;           // super-block scale
} block_q6_K;

然后有这个函数，用于执行反量化操作：

void dequantize_row_q6_K(const block_q6_K * restrict x, float * restrict y, int k) {
    assert(k % QK_K == 0);
    const int nb = k / QK_K;

    for (int i = 0; i < nb; i++) {

        const float d = GGML_FP16_TO_FP32(x[i].d);

        const uint8_t * restrict ql = x[i].ql;
        const uint8_t * restrict qh = x[i].qh;
        const int8_t  * restrict sc = x[i].scales;
但是，我们还是一步一步来看。

# 全知还是鹦鹉？

在这股新一轮的机器学习热潮和进步中，最令人担忧的现象之一是AI专家们在接受自己知识有限方面的能力有限。智人发明了神经网络，然后更关键的是，发明了一种自动优化神经网络参数的算法。硬件已经能够训练越来越大的模型，并利用对待处理数据的统计知识（先验知识）以及通过大量的尝试和错误进行连续逼近，发现了比其他架构更好的架构。但总的来说，神经网络仍然相当不透明。
for (int l = 0; l < 32; ++l) {
                int is = l/16;
                const int8_t q1 = (int8_t)((ql[l +  0] & 0xF) | (((qh[l] >> 0) & 3) << 4)) - 32;
                const int8_t q2 = (int8_t)((ql[l + 32] & 0xF) | (((qh[l] >> 2) & 3) << 4)) - 32;
                const int8_t q3 = (int8_t)((ql[l +  0]  >> 4) | (((qh[l] >> 4) & 3) << 4)) - 32;
                const int8_t q4 = (int8_t)((ql[l + 32]  >> 4) | (((qh[l] >> 6) & 3) << 4)) - 32;
                y[l +  0] = d * sc[is + 0] * q1;
                y[l + 32] = d * sc[is + 2] * q2;
                y[l + 64] = d * sc[is + 4] * q3;
                y[l + 96] = d * sc[is + 6] * q4;
            }
            y  += 128;
            ql += 64;
            qh += 32;
            sc += 8;
        }
    }
}

对于整数 l，当 l 的值在 0 到 32 之间时，执行以下操作：
    计算整数 is，其值为 l 除以 16 的商；
    计算常量 int8_t 类型的 q1，其值为 ql[l + 0] 与 0xF 的按位与，再与 qh[l] 右移 0 位后与 3 的按位与，再左移 4 位后的按位或，最后减去 32；
    计算常量 int8_t 类型的 q2，其值为 ql[l + 32] 与 0xF 的按位与，再与 qh[l] 右移 2 位后与 3 的按位与，再左移 4 位后的按位或，最后减去 32；
    计算常量 int8_t 类型的 q3，其值为 ql[l + 0] 右移 4 位后，再与 qh[l] 右移 4 位后与 3 的按位与，再左移 4 位后的按位或，最后减去 32；
    计算常量 int8_t 类型的 q4，其值为 ql[l + 32] 右移 4 位后，再与 qh[l] 右移 6 位后与 3 的按位与，再左移 4 位后的按位或，最后减去 32；
    计算 y[l + 0]，其值为 d 乘以 sc[is + 0] 乘以 q1；
    计算 y[l + 32]，其值为 d 乘以 sc[is + 2] 乘以 q2；
    计算 y[l + 64]，其值为 d 乘以 sc[is + 4] 乘以 q3；
    计算 y[l + 96]，其值为 d 乘以 sc[is + 6] 乘以 q4；
将 y 的值增加 128；
将 ql 的值增加 64；
将 qh 的值增加 32；
将 sc 的值增加 8。
顺便说一下，这是我自己最后写的代码：

    } else if (tensor->type == GGUF_TYPE_Q6_K) {
        uint8_t *block = (uint8_t*)tensor->weights_data;
        uint64_t i = 0; // i-th weight to dequantize.
        while(i < tensor->num_weights) {
            float super_scale = from_half(*((uint16_t*)(block+128+64+16)));
            uint8_t *L = block;
            uint8_t *H = block+128;
            int8_t *scales = (int8_t*)block+128+64;
            for (int cluster = 0; cluster < 2; cluster++) {
                for (uint64_t j = 0; j < 128; j++) {
                    f[i] = (super_scale * scales[j/16]) *
                           ((int8_t)
                            ((((L[j%64] >> (j/64*4)) & 0xF) |
                             (((H[j%32] >> (j/32*2)) & 3) << 4)))-32);
                    i++;
                    if (i == tensor->num_weights) return f;
                }
                L += 64;
                H += 32;
                scales += 8;
            }

在翻译过程中，请注意满足以下要求：1）文章中的代码不要翻译，保持代码原状；2）不要对这段文字进行压缩或总结；3）逐句翻译，每一句都尽量保留原意；4）分多个步骤进行翻译，每次翻译一段，每翻译一段就要对齐一次，以保证翻译体量尽量和原文相当，不要过多或过少。
最后，如今不使用LLM进行编程有什么道理呢？向LLM提出正确问题是一项基本技能。练习得越少，就越无法借助AI来改进自己的工作。而且，描述问题的能力在与其他人交流时也很有用。LLM并不是唯一一个有时候不明白我们想说什么的对象。沟通不畅是一个很大的局限，许多程序员尽管在自己的专业领域非常有能力，但沟通能力却非常差。现在谷歌已经无法使用：即使只是将LLM作为一种压缩形式的文档来使用也是个好主意。至于我，我将继续广泛地使用它们。我从未喜欢过学习一个晦涩的通信协议的细节，或者一个想展示自己有多厉害的人编写的库的复杂方法。在我看来，这就像是“垃圾知识”。而LLM每天都越来越多地让我免于这些烦恼。
与此同时，许多热情洋溢的群众将一些现实中并不存在的超自然能力归功于LLM。不幸的是，LLM最多只能在它们在训练过程中看到的数据所代表的空间内进行插值：而这已经很多了。实际上，它们的插值能力是有限的（但仍然令人惊讶，也是出乎意料的）。哦，如果今天最大的LLM能够在它们所看到的所有代码的边界内连续插值就好了！即使它们无法产生真正的新颖性，它们也能取代99%的程序员。现实总是更为谦逊，就像它几乎总是这样。LLM确实有能力编写它没有以那种确切形式见过的程序，表现出一定程度的将训练集中出现的不同思想融合在一起的能力。目前，这种能力的局限性也是显而易见的，只要需要微妙的推理，LLM就会失败得很惨。然而，它们代表了从AI诞生至今的最伟大成就。这似乎是不可否认的。
# 愚蠢但无所不知

事实上：LLMs 最多只能进行基本的推理，往往不准确，很多时候还夹杂着对不存在的事实的幻觉。但是他们拥有丰富的知识。在编程领域，以及其他有质量数据可用的领域，LLMs 就像是知道很多事情的愚蠢的天才。与这样的伙伴进行配对编程将是非常糟糕的（对我来说，即使在最一般的情况下，配对编程也是很糟糕的）：他们会有荒谬的想法，我们不得不不断地斗争以实施我们自己的想法。但是，如果这个博学的傻瓜听从我们的安排并回答所有提出的问题，情况就会发生变化。目前的 LLMs 不会让我们走出知识的困境，但是如果我们想要解决一个我们不太了解的话题，他们往往可以帮助我们从绝对无知的状态提升到足够的知识水平，让我们能够自己继续前进。
让我举个例子：我在机器学习方面的实验至少用了一年的Keras。然后由于各种原因，我转向了PyTorch。我已经知道什么是嵌入或残差网络，但我不想逐步研究PyTorch的文档（就像我之前学习Keras时做的那样，那时ChatGPT还不存在）。通过使用LLMs，编写使用Torch的Python代码变得非常容易。我只需要对我想要组合的模型有清晰的想法，并提出正确的问题。

# 示例时间
这是另一个例子。不久前，我需要为某些基于ESP32的设备实现一个BLE客户端。在进行了一些研究之后，我意识到多平台蓝牙编程绑定基本上都是不可用的。解决方案很简单，使用macOS的原生API用Objective C编写代码。因此，我发现自己不得不同时处理两个问题：学习繁琐的Objective C的BLE API，充满了我认为毫无意义的模式（我是个极简主义者，这种API与我认为的“好设计”的光谱的另一端相差甚远），同时还要记住如何用Objective C编程。我上一次用Objective C编写程序是十年前：我不记得事件循环、内存管理等细节。

最终的结果是这段代码，虽然不是很美观，但它完成了它应该做的事情。我在极短的时间内完成了它。否则是不可能的。
在两个 '---' 之间的内容是一篇英文文章，请在尽量保留原意和原本语气的基础上，将它翻译为中文：

首先，我们从预订数据中获取所有独特的房源和年份，然后创建一个包含所有月份的范围。代码如下：

```
all_listings = reservations['Listing'].unique()
all_years = reservations['Year'].unique()
all_months = range(1, 13)
```

接下来，我们使用这些数据创建一个多级索引，其中包含房源、年份和月份的所有可能组合。然后，我们将这个多级索引转换为一个数据框。代码如下：

```
index = pd.MultiIndex.from_product([all_listings, all_years, all_months], names=['Listing', 'Year', 'Month'])
all_data = pd.DataFrame(index=index).reset_index()
```

现在，我们将这个新创建的数据框与原始预订数据合并，根据房源、年份和月份进行合并，并使用左连接来确保所有可能的组合都包含在结果中。代码如下：

```
merged_data = pd.merge(all_data, reservations, on=['Listing', 'Year', 'Month'], how='left')
```

最后，我们根据房源、年份和月份对合并后的数据进行分组，并计算每组的平均每晚价格。如果某个组合没有预订数据，我们用0填充。然后，我们打印出计算得到的平均每晚价格。代码如下：

```
average_nightly_rates = merged_data.groupby(['Listing', 'Year', 'Month'])['Nightly Rate'].mean().fillna(0)
print(average_nightly_rates)
```
这段代码主要是通过在ChatGPT上剪切和粘贴我想要实现但不太了解如何实现的功能来编写的，因此它们并没有正常工作。让LLM向我解释问题所在以及如何解决它。确实，LLM并没有编写很多这样的代码，但它也确实显著加快了编写速度。如果没有ChatGPT，我能完成这个任务吗？当然可以，但最有趣的不是这会花费我更长的时间：事实是，我甚至不会尝试，因为这不值得。这个事实至关重要。编写这样一个与我的项目次要相关的程序所需的努力与收益之比是不划算的。此外，这比程序本身产生了更有用的次要附带效果：为了那个项目，我修改了linenoise（我用于行编辑的库之一），使其能够在多路复用中工作。
# 一次性程序

我可以记录几十个我在上面叙述的这样的案例。这样做是没有意义的，因为这是同一个故事以或多或少相同的方式重复上演。我遇到一个问题，我需要快速了解一些*我可以验证*的东西，以判断LLM是否在给我灌输胡言乱语。在这种情况下，我使用LLM来加速我的知识需求。

然而，还有一些不同的情况，我让LLM编写所有代码。例如，当我需要编写一个或多或少一次性的程序时。就像这个：

https://github.com/antirez/simple-language-model/blob/main/plot.py

---
同样地，我需要一个程序来读取AirBnB的CSV报告，并按月和年对我的公寓进行分组。然后，考虑到清洁费用和每次预订的晚数，它将对不同月份的平均租金价格进行统计。这个程序对我来说非常有用。与此同时，编写它却极度无聊：没有什么有趣的地方。所以我从CSV文件中找了一个很好的部分，然后在GPT4上进行了复制粘贴。我向LLM写明了需要解决的问题：程序在第一次尝试时就成功运行了。我将完整的程序展示在下面。

```python
import pandas as pd
pd.set_option('display.max_rows', None)
df = pd.read_csv('listings.csv')
reservations = df[df['Type'] == 'Reservation']
reservations['Start Date'] = pd.to_datetime(reservations['Start Date'])
reservations['Year'] = reservations['Start Date'].dt.year
reservations['Month'] = reservations['Start Date'].dt.month
```
在我看来，编写这种类型的程序将是对我的时间的不明智使用。事实上，LLM能够为我完成这项工作，这无疑加快了我以一种相当直接的方式编写代码的能力：我可以专注于真正重要的事情。

# 当事情没有按计划进行：系统编程

尽管我在LLM和编程方面取得了成功，但当我用C语言编写程序时，我几乎总是只把LLM当作一种更方便的文档形式来使用。我是一名系统编程专家，在这种情况下，LLM只有在能够进行复杂推理的情况下才对我有用：在这种情况下，它们几乎总是失败的。

让我们从这个实验的提示开始：

概述一个简洁、小巧但可用的C语言实现的布隆过滤器的需求。重点关注哈希函数的重要方面。然后用高质量的C语言编写它。
以下是输出内容：

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>

#define N 100000  // 元素数量
#define P 0.05    // 假阳性概率
#define M (int)(-(N * log(P)) / (log(2) * log(2)))  // 位数组大小
#define K (int)(M / N * log(2))  // 哈希函数数量

unsigned char bitArray[M / 8 + 1];

void setBit(int n) {
    bitArray[n / 8] |= 1 << (n % 8);
}

int getBit(int n) {
    return bitArray[n / 8] & (1 << (n % 8));
}

// 基本哈希函数（用于演示目的）
int hash1(char *str) {
    int hash = 5381;
    int c;
    while ((c = *str++))
        hash = ((hash << 5) + hash) + c;
    return abs(hash % M);
}

int hash2(char *str) {
    unsigned long hash = 0;
    int c;
    while ((c = *str++))
        hash = c + (hash << 6) + (hash << 16) - hash;
    return abs(hash % M);
}

void add(char *str) {
    setBit(hash1(str));
    setBit(hash2(str));
    // 如果需要，添加更多哈希函数
}

```
```c
    return getBit(hash1(str)) && getBit(hash2(str));
    // 如果使用了更多的哈希函数，请检查它们
}

int main() {
    char *testStr = "testString";
    add(testStr);

    if (is_in(testStr)) {
        printf("%s is probably in the set.\n", testStr);
    } else {
        printf("%s is definitely not in the set.\n", testStr);
    }

    return 0;
}
```

GPT4生成的实现并不是特别出色。布隆过滤器应用非常广泛，它并不是一种特别奇特的数据结构。然而，很明显，编写一个像样的布隆过滤器需要更强的抽象能力：例如，如何找到一种有效的方法来对同一个字符串进行N次哈希，以及确保这些哈希值具有足够的解相关性。

然而，如果你明确要求GPT4修改哈希函数，使其产生N个解相关的输出，它提出的解决方案会显得更加可接受：

```c
unsigned int hash(const char* str, unsigned int hash_id) {
```
```
int c;
while ((c = *str++)) {
    hash = ((hash << 5) + hash) + c + hash_id;
}
return hash % BF_SIZE;
}
```

如果它自己想出了这个想法，它会用不同的方式编写布隆过滤器，使用单个哈希函数一次设置K个位。

GPT4能够独立完成编写一个合适且更通用的哈希函数的任务。但在编写布隆过滤器的更大项目中，它未能展示出足够的推理能力，而是输出了两个不同但相似的哈希函数：这正是人们从一篇关于布隆过滤器的糟糕文章示例中所期望的。
与此同时，GPT-4 能够反编译它生成的函数（在另一个会话中），并且非常准确地理解其目的：

https://chat.openai.com/share/f54b64d7-d19a-4be4-8f10-08553aa812f1

因此，声称在系统编程环境中没有用途显然是不准确的。

有趣的是，在这些情况下，较小模型和较大模型之间的差异确实很大。让我们看看两个本地模型的表现。

例如，Mixtral（Mistral 的 MoE）对同一个问题作出了如下回答：

```c
unsigned int hash(const char* str, unsigned int hash_id) {
    unsigned int hash = 5381;
    int c;
    while ((c = *str++)) {
        hash = ((hash << 5) + hash) + c;
    }
    return hash + hash_id;
}
```
这是与当地LLM深度寻求编码器的一次聊天，由于我没有足够的RAM来使用更高精度的模型，所以将其量化为4位。然而，由于其具有340亿个参数，因此在这个问题上的推理能力似乎相当先进。以下是聊天记录（使用llama.cpp进行）：

> 我有一个哈希函数，它应该为相同的数据输出N个不同的哈希值。然而，当我改变hash_id的值时，我发现生成的哈希值的分布非常糟糕：

unsigned int hash(const char* str, unsigned int hash_id) {
    unsigned int hash = 5381;
    int c;
    while ((c = *str++)) {
        hash = ((hash << 5) + hash) + c;
    }
    return hash + hash_id;
}

也许我使用的加盐策略不好？我应该如何改变它？

[深度寻求编码器回复]

你实现的一个可能问题是，你只是将`hash_id`添加到哈希的末尾。这可能导致分布很差，特别是如果`hash_id`太大或者分布不均匀。
在混合 `hash_id` 时，可以考虑使用位运算。以下是一个使用异或（XOR）的示例：

```c
unsigned int hash(const char* str, unsigned int hash_id) {
    unsigned int hash = 5381;
    int c;
    while ((c = *str++)) {
        hash = ((hash << 5) + hash) ^ c + hash_id; // 使用 XOR 将 hash_id 混入
     }
    return (hash ^ hash_id); // 在最后再次混入 hash_id
}
```

这将比简单地将其添加到哈希值中更好地将 `hash_id` 散布在整个哈希值中，从而可能导致分布不太均匀。[剪辑...实际回复包含了更多有益的信息]
