遗憾地说，但这是事实：如今的大部分编程工作都是以略有不同的形式重复同样的事情。这并不需要高水平的推理能力。LLM在这方面做得相当不错，尽管它们仍然受到上下文最大尺寸的严重限制。这真的应该让程序员们思考一下。编写这种类型的程序值得吗？当然，你可以得到丰厚的报酬，但如果LLM可以完成其中的一部分，也许在五到十年后，这并不是最好的位置。

那么，LLM是否具有一些推理能力，还是这一切都只是虚张声势？也许有时候，它们之所以看似具有推理能力，只是因为，正如符号学家所说，"能指"给人一种实际上并不存在的意义的印象。那些与LLM共事足够多的人，在接受它们的局限性的同时，肯定知道事实并非如此：它们将所见过的东西融合在一起的能力远远超过了随机地反复吐露词汇。尽管它们的训练主要是在预训练阶段进行的，目标是预测下一个标记，但这个目标迫使模型创建某种抽象模型。这个模型虽然脆弱、不完整且不完美，但如果我们观察到了我们所观察到的现象，那么它必须存在。如果我们的数学确定性是可疑的，而最伟大的专家往往持相反的立场，那么相信自己亲眼所见似乎是明智的做法。

最后，如今不使用LLM进行编程有什么意义呢？向LLM提出正确问题是一项基本技能。这项技能练得越少，人们就越难以借助AI来改进他们的工作。而且，描述问题的能力在与其他人交流时也很有用。LLM并不是唯一一个有时候不明白我们想说什么的人。沟通不良是一个很大的局限，许多程序员尽管在他们的专业领域非常有能力，但沟通能力却非常差。而现在谷歌已经无法使用：即使只是将LLM作为一种压缩形式的文档，也是一个好主意。至于我，我将继续广泛地使用它们。我从未喜欢过学习一个晦涩的通信协议的细节，或者一个想展示自己有多厉害的人编写的库的复杂方法。对我来说，这似乎是“垃圾知识”。而LLM每天都越来越多地将我从这一切中解救出来。