遗憾地说，这是事实：如今的大部分编程工作都是以略有不同的形式重复同样的事情。这并不需要高水平的推理能力。LLM（大型语言模型）在这方面做得相当不错，尽管它们仍然受到上下文最大尺寸的严重限制。这应该让程序员们思考一下。编写这类程序值得吗？当然，你可以得到丰厚的报酬，但如果LLM可以完成其中的一部分，也许在五到十年后，这并不是最好的位置。

那么，LLM是否具有一定的推理能力，还是只是在虚张声势？也许有时候，它们之所以看似具有推理能力，只是因为，正如符号学家所说，"能指"给人一种实际上并不存在的意义的印象。那些与LLM共事足够长时间并接受它们的局限性的人肯定知道这不可能是真的：它们将所见过的东西融合在一起的能力远远超过了随机地重复单词。尽管它们的训练主要在预训练阶段进行，以预测下一个标记，但这个目标迫使模型创建某种抽象模型。这个模型虽然薄弱、不完整且不完美，但如果我们观察到了我们所观察到的现象，那么它必须存在。如果我们的数学确定性是可疑的，而最伟大的专家往往持相反的立场，相信自己亲眼所见似乎是明智的做法。

最后，如今不使用LLM进行编程有什么意义呢？向LLM提出正确问题是一项基本技能。练习得越少，就越难以借助AI改进工作。此外，描述问题的能力在与其他人交流时也很有用。LLM并不是唯一有时候无法理解我们想说什么的对象。沟通不良是一个很大的局限，许多程序员尽管在特定领域非常有能力，但沟通能力很差。现在谷歌已经无法使用：即使只是将LLM用作压缩形式的文档也是个好主意。就我个人而言，我将继续广泛使用它们。我从未喜欢过学习一个晦涩的通信协议的细节，或者一个想展示自己有多厉害的人编写的库的复杂方法。对我来说，这似乎是“垃圾知识”。LLM每天都在越来越多地将我从这些事情中解救出来。