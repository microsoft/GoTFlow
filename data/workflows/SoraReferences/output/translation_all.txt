1. 题名：使用LSTM对视频表示进行无监督学习
作者：Nitish Srivastava，Elman Mansimov，Ruslan Salakhutdinov
摘要：我们使用多层长短时记忆（LSTM）网络来学习视频序列的表示。我们的模型使用一个编码器LSTM将输入序列映射到一个固定长度的表示。这个表示使用单个或多个解码器LSTM进行解码以执行不同的任务，例如重构输入序列或预测未来序列。我们尝试两种输入序列 - 图像像素块和使用预训练卷积网络提取的视频帧的高级表示（“感知”）。我们探讨了不同的设计选择，例如解码器LSTM是否应该基于生成的输出。我们从定性的角度分析模型的输出，以了解模型如何将学到的视频表示推广到未来和过去。我们尝试可视化和解释学到的特征。我们通过在更长的时间尺度和领域外数据上运行模型来对其进行压力测试。我们进一步通过对UCF-101和HMDB-51数据集上的人类动作识别进行微调来评估表示。我们证明了这些表示有助于提高分类准确性，尤其是在只有少量训练样本的情况下。即使是在与任务无关的数据集（YouTube视频的300小时）上预训练的模型也可以帮助提高动作识别性能。
---
2. 题目：循环环境模拟器
作者：Silvia Chiappa, Sébastien Racaniere, Daan Wierstra & Shakir Mohamed
摘要：能够模拟环境如何根据行动而变化的模型可以被代理用来有效地规划和行动。我们通过引入能够对未来数百个时间步进行时间和空间连贯预测的循环神经网络，改进了以前从高维像素观测中得到的环境模拟器。我们对影响性能的因素进行了深入分析，提供了最广泛的尝试，以提高对这些模型属性的理解。我们通过一个不需要在每个时间步生成高维图像的模型来解决计算效率低的问题。我们证明了我们的方法可以用于改进探索，并且可以适应许多不同的环境，即10个Atari游戏，一个3D赛车环境和复杂的3D迷宫。
---
---
3. 题目：世界模型
作者：David Ha，Jurgen Schmidhuber
摘要：我们探讨了构建基于流行强化学习环境的生成神经网络模型。我们的世界模型可以通过无监督的方式快速训练，学习环境的压缩空间和时间表示。通过将世界模型中提取的特征作为代理输入，我们可以训练一个非常紧凑且简单的策略来解决所需任务。我们甚至可以完全在代理自己的世界模型生成的幻觉梦境中训练代理，并将此策略转移到实际环境中。本文的交互式版本可在 https://worldmodels.github.io 获取。
---
---
4. 题目：生成具有场景动态的视频
作者：Carl Vondrick，Hamed Pirsiavash，Antonio Torralba
摘要：我们利用大量未标记的视频来学习场景动态模型，以便用于视频识别任务（如动作分类）和视频生成任务（如未来预测）。我们提出了一种具有时空卷积结构的视频生成对抗网络，该结构可以将场景的前景与背景分离。实验表明，该模型可以比简单基线更好地生成长达一秒的全帧速率的微小视频，并且我们展示了其在预测静态图像的合理未来方面的实用性。此外，实验和可视化表明，该模型在最小监督下内部学习识别动作的有用特征，表明场景动态是一种有前景的表示学习信号。我们相信生成视频模型可以影响视频理解和模拟的许多应用。
---
5. 题目：MoCoGAN：将运动和内容分解用于视频生成
作者：Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz
摘要：视频中的视觉信号可以分为内容和运动。内容指定了视频中的对象，而运动描述了它们的动态。基于这个先验，我们提出了一种用于视频生成的运动和内容分解生成对抗网络（MoCoGAN）框架。所提出的框架通过将一系列随机向量映射到一系列视频帧来生成视频。每个随机向量包括内容部分和运动部分。在保持内容部分固定的同时，将运动部分实现为随机过程。为了在无监督的情况下学习运动和内容的分解，我们引入了一种新颖的对抗学习方案，利用图像和视频鉴别器。在几个具有挑战性的数据集上进行的广泛实验结果，以及与最先进方法的定性和定量比较，验证了所提出框架的有效性。此外，我们还展示了MoCoGAN允许生成具有相同内容但不同运动的视频，以及具有不同内容和相同运动的视频。
---
6. 题目：在复杂数据集上的对抗性视频生成
作者：Aidan Clark，Jeff Donahue，Karen Simonyan
摘要：通过强大的尺度杠杆作用，自然图像的生成模型已经取得了高保真度样本的进展。我们试图将这一成功应用到视频建模领域，通过在复杂的Kinetics-600数据集上训练大型生成对抗网络（GAN），生成的视频样本的复杂性和保真度明显高于以前的工作。我们提出的模型，双视频鉴别器生成对抗网络（DVD-GAN），通过利用计算效率高的鉴别器分解，实现了更长、更高分辨率的视频生成。我们在视频合成和视频预测的相关任务上进行评估，并在Kinetics600上实现了新的最先进的Fréchet Inception距离预测，以及在UCF-101数据集上实现了最先进的Inception评分合成，同时在Kinetics-600上建立了强大的合成基线。
---
---
7. 题目：生成动态场景的长视频
作者：Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei A. Efros, Tero Karras
摘要：我们提出了一个视频生成模型，可以准确地再现物体运动、摄像机视角的变化以及随时间产生的新内容。现有的视频生成方法通常无法在保持真实环境中预期的一致性（如合理的动态和物体持续性）的同时，随时间产生新内容。一个常见的失败案例是由于过度依赖归纳偏差来提供时间一致性，例如一个单一的潜在代码决定整个视频的内容，导致内容永远不会发生变化。在另一个极端情况下，如果没有长期一致性，生成的视频可能会在不同的场景之间不现实地变形。为了解决这些局限性，我们通过重新设计时间潜在表示并通过在更长的视频上进行训练来从数据中学习长期一致性。为此，我们利用了两阶段训练策略，分别在低分辨率的长视频和高分辨率的短视频上进行训练。为了评估我们模型的能力，我们引入了两个新的基准数据集，专注于长期时间动态。
---
---
8. 题目：VideoGPT：使用VQ-VAE和Transformer的视频生成
作者：Wilson Yan, Yunzhi Zhang, Pieter Abbeel, Aravind Srinivas
摘要：我们提出了VideoGPT：一种概念简单的架构，用于将基于似然的生成建模扩展到自然视频。VideoGPT使用VQ-VAE通过采用3D卷积和轴向自注意力来学习原始视频的下采样离散潜在表示。然后使用简单的GPT类似架构通过时空位置编码自回归地对离散潜变量进行建模。尽管在公式化和训练方便性上具有简单性，但我们的架构能够在BAIR机器人数据集上生成与最先进的GAN模型竞争的样本，并从UCF-101和Tumbler GIF数据集（TGIF）生成高保真度的自然视频。我们希望我们提出的架构能作为基于Transformer的视频生成模型的简约实现的可复制参考。样本和代码可在https://wilson1yan.github.io/videogpt/index.html 上找到。
---
---
9. 题目：NU¨ WA：用于神经视觉世界创建的视觉合成预训练
作者：吴晨飞，梁健，季磊，杨帆，方跃建，蒋大鑫，段楠
摘要：本文提出了一个统一的多模态预训练模型NÜWA，可以为各种视觉合成任务生成新的或操纵现有的视觉数据（即图像和视频）。为了同时涵盖不同场景下的语言、图像和视频，我们设计了一个3D变压器编码器-解码器框架，它不仅可以处理视频作为3D数据，还可以分别适应文本和图像作为1D和2D数据。我们还提出了一种3D近邻注意力（3DNA）机制，以考虑视觉数据的性质并降低计算复杂性。我们在8个下游任务上评估NÜWA。与几个强基线相比，NÜWA在文本到图像生成、文本到视频生成、视频预测等方面取得了最先进的结果。此外，它在文本引导的图像和视频操纵任务上还表现出令人惊讶的零样本能力。
---
10. 题目：Imagen Video：使用扩散模型生成高清视频
作者：Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, Tim Salimans
摘要：我们提出了Imagen Video，这是一个基于视频扩散模型级联的文本条件视频生成系统。给定一个文本提示，Imagen Video使用基础视频生成模型和一系列交错的空间和时间视频超分辨率模型生成高清视频。我们描述了如何将系统扩展为高清文本到视频模型，包括设计决策，如在某些分辨率下选择完全卷积的时间和空间超分辨率模型，以及选择扩散模型的v参数化。此外，我们确认并将扩散为基础的图像生成的先前工作的发现转移到视频生成设置。最后，我们在无分类器引导下对视频模型应用渐进式蒸馏，以实现快速、高质量的采样。我们发现Imagen Video不仅能生成高保真度的视频，而且具有高度的可控性和世界知识，包括生成各种艺术风格的多样化视频和文本动画以及对3D物体理解的能力。请访问https://imagen.research.google/video/ 查看样本。

11. 题目：对齐潜在变量：使用潜在扩散模型进行高分辨率视频合成
作者：Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis
摘要：潜在扩散模型（LDMs）通过在压缩的低维潜在空间中训练扩散模型，实现高质量图像合成，同时避免过多的计算需求。在这里，我们将LDM范式应用于高分辨率视频生成，这是一项特别消耗资源的任务。我们首先仅在图像上预训练一个LDM；然后，通过在潜在空间扩散模型中引入时间维度，并在编码的图像序列（即视频）上进行微调，将图像生成器变为视频生成器。类似地，我们将扩散模型上采样器在时间上对齐，将它们变成具有时间一致性的视频超分辨率模型。我们关注两个相关的现实世界应用：模拟野外驾驶数据和通过文本到视频建模的创意内容创建。特别是，我们在分辨率为512 x 1024的真实驾驶视频上验证了我们的视频LDM，实现了最先进的性能。此外，我们的方法可以轻松利用现成的预训练图像LDM，因为在这种情况下，我们只需要训练一个时间对齐模型。通过这样做，我们将公开可用的最先进的文本到图像LDM稳定扩散转变为一种高效且富有表现力的文本到视频模型，分辨率可达1280 x 2048。我们证明了以这种方式训练的时间层可以推广到不同的微调文本到图像LDM。利用这一特性，我们展示了个性化文本到视频生成的第一个结果，为未来内容创建开辟了令人兴奋的方向。
---
12. 题目：使用扩散模型进行真实感视频生成
作者：Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, José Lezama
摘要：我们提出了一种基于变压器的方法W.A.L.T，通过扩散建模实现真实感视频生成。我们的方法有两个关键设计决策。首先，我们使用因果编码器将图像和视频共同压缩到一个统一的潜在空间中，实现跨模态的训练和生成。其次，为了提高内存和训练效率，我们采用了一种专门针对联合空间和时空生成建模的窗口注意力架构。这些设计决策使我们能够在已建立的视频（UCF-101和Kinetics-600）和图像（ImageNet）生成基准上实现最先进的性能，而无需使用分类器自由引导。最后，我们还训练了一个由三个模型组成的级联，用于文本到视频生成任务，包括一个基本的潜在视频扩散模型，以及两个视频超分辨率扩散模型，以每秒8帧的速度生成512×896分辨率的视频。
---
---
13. 题目：只需关注力
作者：Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
摘要：主导的序列转换模型基于复杂的循环或卷积神经网络，采用编码器-解码器配置。性能最好的模型还通过关注机制将编码器和解码器连接起来。我们提出了一种新的简单网络架构，即Transformer，它完全基于关注机制，摒弃了循环和卷积。在两个机器翻译任务的实验中，这些模型在质量上表现出优越性，同时具有更高的并行性，训练时间明显减少。我们的模型在WMT 2014英语-德语翻译任务上实现了28.4 BLEU，比现有的最佳结果（包括集成）提高了2个BLEU。在WMT 2014英语-法语翻译任务上，我们的模型在八个GPU上训练3.5天后，以41.8的BLEU分数创下了新的单模型最高水平，这只是文献中最佳模型的训练成本的一小部分。我们通过将其成功应用于英语成分解析（包括大量和有限的训练数据），证明了Transformer对其他任务具有很好的泛化能力。
---
---
14. 题目：语言模型是少样本学习器
作者：Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei
摘要：最近的研究表明，在大量文本语料库上进行预训练，然后在特定任务上进行微调，可以在许多自然语言处理（NLP）任务和基准测试中取得显著提升。尽管这种方法在架构上通常是任务无关的，但仍需要数千或数万个示例的任务特定微调数据集。相比之下，人类通常只需要几个示例或简单的指令就可以完成新的语言任务，而当前的NLP系统在这方面仍然存在很大的困难。在这里，我们展示了通过扩大语言模型可以显著提高任务无关的少样本性能，有时甚至可以达到与之前最先进的微调方法相媲美的水平。具体来说，我们训练了GPT-3，一个具有1750亿参数的自回归语言模型，比以前的任何非稀疏语言模型多10倍，并在少样本设置中测试其性能。对于所有任务，GPT-3都是在没有梯度更新或微调的情况下应用的，任务和少样本演示仅通过与模型的文本交互来指定。GPT-3在许多NLP数据集上取得了很好的性能，包括翻译、问答和填空任务，以及一些需要实时推理或领域适应的任务，如对单词进行重新排序、在句子中使用新词或进行3位数的算术运算。与此同时，我们还发现了一些GPT-3在少样本学习中仍然存在困难的数据集，以及一些GPT-3在大型网络语料库上训练时面临的方法论问题。最后，我们发现GPT-3可以生成新闻文章样本，人类评估者很难区分这些文章是由人类还是机器生成的。我们讨论了这一发现以及GPT-3总体上对社会的更广泛影响。
---
---
15. 题目：一幅图像价值16x16个单词：大规模图像识别的变压器
作者：Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby
摘要：尽管Transformer架构已经成为自然语言处理任务的事实标准，但其在计算机视觉领域的应用仍然有限。在视觉领域，注意力机制要么与卷积网络结合使用，要么用于替换卷积网络的某些组件，同时保持其整体结构。我们证明，这种依赖CNN的做法是没有必要的，直接将纯Transformer应用于图像块序列在图像分类任务上表现非常出色。当在大量数据上进行预训练并转移到多个中等规模或小规模的图像识别基准（ImageNet，CIFAR-100，VTAB等）时，视觉变压器（ViT）与最先进的卷积网络相比，取得了优异的结果，同时在训练过程中需要的计算资源要少得多。
---
---
16. 题目：ViViT：视频视觉变换器
作者：Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid
摘要：我们提出了一种基于纯变换器的视频分类模型，借鉴了最近这种模型在图像分类中的成功。我们的模型从输入视频中提取时空标记，然后通过一系列变换器层进行编码。为了处理视频中遇到的长序列标记，我们提出了几种有效的模型变体，它们将输入的空间和时间维度进行因子分解。尽管已知基于变换器的模型仅在大型训练数据集可用时才有效，但我们展示了如何在训练过程中有效地对模型进行正则化，并利用预训练的图像模型来处理相对较小的数据集。我们进行了详尽的消融研究，并在多个视频分类基准测试中取得了最先进的结果，包括Kinetics 400和600、Epic Kitchens、Something-Something v2和Moments in Time，优于基于深度3D卷积网络的先前方法。为了促进进一步的研究，我们在此https URL发布了代码。
---
17. 题名：遮罩自编码器是可扩展的视觉学习器
作者：何凯明，陈新雷，谢赛宁，李阳昊，皮奥特·多拉尔，罗斯·吉尔希克
摘要：本文表明，遮罩自编码器（MAE）是可扩展的自监督计算机视觉学习器。我们的MAE方法很简单：我们遮住输入图像的随机区域，并重建缺失的像素。它基于两个核心设计。首先，我们开发了一个非对称的编码器-解码器架构，编码器仅在可见的子区域上操作（不包括遮罩标记），而轻量级的解码器从潜在表示和遮罩标记中重建原始图像。其次，我们发现遮住大部分输入图像（例如75%）会产生一个非平凡且有意义的自监督任务。将这两个设计结合起来，使我们能够高效有效地训练大型模型：我们加速训练（提高3倍或更多）并提高准确性。我们的可扩展方法允许学习高容量模型，具有良好的泛化能力：例如，一个普通的ViT-Huge模型在仅使用ImageNet-1K数据的方法中获得了最高的准确率（87.8%）。在下游任务中的迁移性能超过了有监督的预训练，并显示出有希望的扩展行为。
---
18. 题目：Patch n' Pack：NaViT，一种适用于任何纵横比和分辨率的视觉变换器
作者：Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, Avital Oliver, Piotr Padlewski, Alexey Gritsenko, Mario Lučić, Neil Houlsby
摘要：在计算机视觉模型处理图像之前，将图像调整为固定分辨率的做法无处不在，但其效果明显不理想，尚未成功受到挑战。然而，像视觉变换器（ViT）这样的模型提供了灵活的基于序列的建模，因此可以处理不同长度的输入序列。我们利用这一点，开发了NaViT（原生分辨率ViT），在训练过程中使用序列打包来处理任意分辨率和纵横比的输入。除了灵活的模型使用外，我们还展示了大规模监督和对比图像文本预训练的改进训练效率。NaViT可以高效地转移到标准任务，如图像和视频分类、目标检测和语义分割，并在鲁棒性和公平性基准测试中取得改进的结果。在推理时，输入分辨率的灵活性可以用来平滑地导航测试时的成本-性能权衡。我们相信，NaViT标志着与大多数计算机视觉模型使用的标准CNN设计输入和建模流程的分离，并代表了ViT的一个有前景的发展方向。
---
19. 题目：高分辨率图像合成的潜在扩散模型
作者：Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer
摘要：通过将图像形成过程分解为去噪自动编码器的顺序应用，扩散模型（DMs）在图像数据及其他方面实现了最先进的合成结果。此外，它们的公式允许引入一个引导机制来控制图像生成过程，而无需重新训练。然而，由于这些模型通常直接在像素空间中操作，优化强大的DMs通常需要数百个GPU天，并且由于顺序评估，推理成本很高。为了在有限的计算资源上进行DM训练，同时保留其质量和灵活性，我们将它们应用于强大的预训练自动编码器的潜在空间。与以前的工作相比，对这种表示进行扩散模型的训练首次实现了在复杂性降低和细节保留之间达到近乎最优的平衡，极大地提高了视觉保真度。通过在模型架构中引入交叉注意力层，我们将扩散模型变成了强大且灵活的生成器，可用于处理一般条件输入，如文本或边界框，并以卷积方式实现高分辨率合成。我们的潜在扩散模型（LDMs）在图像修复方面实现了新的最先进水平，并在各种任务上取得了高度竞争性的性能，包括无条件图像生成、语义场景合成和超分辨率，同时与基于像素的DMs相比显著降低了计算需求。代码可在此https URL获取。
---
20. 题目：自动编码变分贝叶斯
作者：Diederik P. Kingma，Max Welling
摘要：在存在具有难以处理的后验分布的连续潜变量和大型数据集的情况下，我们如何在有向概率模型中进行高效的推断和学习？我们引入了一种随机变分推断和学习算法，该算法可以扩展到大型数据集，并且在一些温和的可微条件下，甚至可以在难以处理的情况下工作。我们的贡献有两个方面。首先，我们展示了变分下界的重新参数化产生了一个下界估计器，该估计器可以使用标准的随机梯度方法进行直接优化。其次，我们证明了对于具有每个数据点的连续潜变量的独立同分布数据集，通过使用所提出的下界估计器拟合近似推断模型（也称为识别模型）到难以处理的后验，后验推断可以变得特别高效。理论优势反映在实验结果中。
---
---
21. 题目：使用非平衡热力学进行深度无监督学习
作者：Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli
摘要：机器学习中的一个核心问题是使用高度灵活的概率分布族对复杂数据集进行建模，同时保证学习、抽样、推理和评估在分析或计算上是可行的。在这里，我们开发了一种既灵活又易于处理的方法。这个方法的基本思想受到非平衡统计物理的启发，通过一个迭代的前向扩散过程系统地、缓慢地破坏数据分布中的结构。然后，我们学习一个恢复数据结构的逆扩散过程，从而得到一个高度灵活且易于处理的数据生成模型。这种方法使我们能够快速地学习、抽样和评估具有数千层或时间步的深度生成模型中的概率，以及计算学习模型下的条件概率和后验概率。此外，我们还发布了该算法的开源参考实现。
---
22. 题目：去噪扩散概率模型
作者：何乔纳森，贾因阿贾，阿贝尔皮特
摘要：我们使用扩散概率模型呈现高质量的图像合成结果，这是一类受非平衡热力学考虑的潜在变量模型。我们最好的结果是通过在一个加权变分界限上进行训练得到的，这个界限是根据扩散概率模型与去噪分数匹配的朗之万动力学之间的新颖联系设计的。我们的模型自然地承认一个渐进的有损解压缩方案，可以将其解释为自回归解码的泛化。在无条件的CIFAR10数据集上，我们获得了9.46的Inception分数和3.17的最先进的FID分数。在256x256 LSUN上，我们获得了与ProgressiveGAN相似的样本质量。我们的实现可以在此https URL中找到。
---
23. 题目：改进的去噪扩散概率模型
作者：Alex Nichol, Prafulla Dhariwal
摘要：去噪扩散概率模型（DDPM）是一类生成模型，最近已被证明可以生成优秀的样本。我们证明，通过一些简单的修改，DDPM可以在保持高样本质量的同时，实现具有竞争力的对数似然。此外，我们发现学习反向扩散过程的方差允许在前向传播数量减少一个数量级的情况下，样本质量的差异可以忽略不计，这对于这些模型的实际部署非常重要。我们还使用精确度和召回率来比较DDPM和GAN覆盖目标分布的效果。最后，我们证明这些模型的样本质量和似然性能随着模型容量和训练计算的增加而平滑地提高，使得它们容易扩展。我们在此https URL发布我们的代码。
---
---
24. 题目：扩散模型在图像合成上胜过生成对抗网络
作者：Prafulla Dhariwal, Alex Nichol
摘要：我们证明了扩散模型可以在图像样本质量上超越当前最先进的生成模型。我们通过一系列的消融实验找到了更好的架构，从而在无条件图像合成上实现了这一目标。对于有条件的图像合成，我们进一步通过分类器引导来提高样本质量：这是一种简单、计算高效的方法，通过使用来自分类器的梯度来在多样性和保真度之间进行权衡。我们在ImageNet 128×128上实现了2.97的FID，在ImageNet 256×256上实现了4.59的FID，在ImageNet 512×512上实现了7.72的FID，即使每个样本只有25个前向传播，我们也能与BigGAN-deep相匹敌，同时保持更好的分布覆盖。最后，我们发现分类器引导与上采样扩散模型相结合效果更佳，将ImageNet 256×256的FID进一步提高到3.94，将ImageNet 512×512的FID提高到3.85。我们在此https URL发布了我们的代码。
---
25. 题名：阐述基于扩散的生成模型的设计空间
作者：Tero Karras, Miika Aittala, Timo Aila, Samuli Laine
摘要：我们认为，目前基于扩散的生成模型的理论和实践过于复杂，为了解决这个问题，我们提出了一个设计空间，明确区分了具体的设计选择。这使我们能够对采样和训练过程以及评分网络的预处理进行多项改进。综合起来，我们的改进为CIFAR-10在类条件设置下实现了最新的FID 1.79，无条件设置下实现了1.97，采样速度（每张图像35次网络评估）比之前的设计快得多。为了进一步展示它们的模块化特性，我们展示了我们的设计改进如何显著提高了先前工作中预训练评分网络的效率和质量，包括将先前训练的ImageNet-64模型的FID从2.07提高到接近最新水平的1.55，经过我们建议的改进重新训练后，达到了新的最新水平1.36。
---
26. 题目：基于Transformer的可扩展扩散模型
作者：William Peebles, Saining Xie
摘要：我们探讨了一类基于Transformer架构的扩散模型。我们训练了图像的潜在扩散模型，用在潜在块上操作的Transformer替换了常用的U-Net骨干。我们通过Gflops测量的前向传播复杂性来分析我们的扩散Transformer（DiT）的可扩展性。我们发现，具有更高Gflops的DiT（通过增加Transformer的深度/宽度或增加输入令牌的数量）始终具有较低的FID。除了具有良好的可扩展性属性外，我们最大的DiT-XL/2模型在类条件ImageNet 512x512和256x256基准测试中均优于所有先前的扩散模型，在后者上实现了最先进的FID 2.27。
---
---
27. 题目：像素生成预训练
作者：陈马克，雷德福德，瑞文·查尔德，吴杰，骆禾，普拉富拉·达里瓦尔，卢安·大卫，伊利亚·苏茨克维尔
摘要：受到自然语言无监督表示学习进展的启发，我们研究类似的模型是否可以为图像学习有用的表示。我们训练一个序列Transformer来自动回归地预测像素，而不需要结合二维输入结构的知识。尽管在没有标签的低分辨率ImageNet上进行训练，我们发现GPT-2规模的模型通过线性探测、微调和低数据分类来学习强大的图像表示。在CIFAR-10上，我们通过线性探测实现了96.3%的准确率，超过了监督的Wide ResNet，并通过完全微调实现了99.0%的准确率，与顶级监督预训练模型相匹配。一个在ImageNet和网络图像混合数据上训练的更大模型在ImageNet上与自监督基准竞争，通过我们特征的线性探测实现了72.0%的top-1准确率。
---
---
28. 题目：零样本文本到图像生成
作者：Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever
摘要：传统的文本到图像生成主要关注在固定数据集上寻找更好的建模假设。这些假设可能涉及复杂的架构、辅助损失或在训练过程中提供的对象部分标签或分割掩码等辅助信息。我们为这个任务提出了一种基于变压器的简单方法，该方法自回归地将文本和图像标记作为单一数据流进行建模。在足够的数据和规模下，我们的方法在零样本评估中与之前的领域特定模型具有竞争力。
---
29. 题目：扩展自回归模型用于内容丰富的文本到图像生成
作者：Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu
摘要：我们提出了一种路径自回归文本到图像（Parti）模型，该模型生成高保真度的真实感图像，并支持涉及复杂构图和世界知识的内容丰富的合成。Parti将文本到图像生成视为一个序列到序列建模问题，类似于机器翻译，目标输出是图像标记序列而不是另一种语言的文本标记。这种策略可以自然地利用大型语言模型的丰富成果，这些模型通过扩大数据和模型规模不断提高性能和能力。我们的方法很简单：首先，Parti使用基于Transformer的图像标记器ViT-VQGAN将图像编码为离散标记的序列。其次，我们通过将编码器-解码器Transformer模型扩展到200亿参数，实现了持续的质量改进，并在MS-COCO上实现了最新的零射击FID得分7.23和微调FID得分3.22。我们对Localized Narratives以及PartiPrompts（P2）进行了详细分析，P2是一种包含超过1600个英语提示的新型全面基准测试，证明了Parti在各种类别和难度方面的有效性。我们还探讨并强调了我们模型的局限性，以确定并举例说明需要进一步改进的关键领域。请访问https://parti.research.google/ 查看高分辨率图像。
---
30. 题目：通过更好的图像描述提高图像生成质量
作者：James Betker, James Betker, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, Aditya Ramesh
摘要：我们证明了通过在高度描述性的生成图像标题上进行训练，可以显著提高文本到图像模型的提示跟随能力。现有的文本到图像模型在遵循详细的图像描述方面存在困难，通常会忽略单词或混淆提示的含义。我们假设这个问题源于训练数据集中的噪声和不准确的图像标题。为解决这个问题，我们训练了一个定制的图像标题生成器，并用它重新生成训练数据集的标题。然后，我们训练了几个文本到图像模型，并发现在这些合成标题上进行训练可以可靠地提高提示跟随能力。最后，我们利用这些发现构建了DALL-E 3：一个新的文本到图像生成系统，并在一个旨在衡量提示跟随、一致性和美学的评估中对其性能进行基准测试，发现它与竞争对手相比具有优势。我们发布了这些评估的样本和代码，以便未来的研究可以继续优化文本到图像系统的这一重要方面。
---
31. 题目：基于CLIP潜变量的分层文本条件图像生成
作者：Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen
摘要：已经证明，像CLIP这样的对比模型可以学习到既包含语义又包含风格的图像的强大表示。为了利用这些表示进行图像生成，我们提出了一个两阶段模型：一个先验模型，根据文本标题生成CLIP图像嵌入；一个解码器，根据图像嵌入生成图像。我们证明，显式生成图像表示可以在保持照片真实性和标题相似性的前提下，提高图像的多样性。我们根据图像表示条件的解码器还可以生成保留其语义和风格的图像变体，同时改变图像表示中不存在的非必要细节。此外，CLIP的联合嵌入空间可以实现零样本的语言引导图像操作。我们使用扩散模型作为解码器，并尝试使用自回归和扩散模型作为先验模型，发现后者在计算效率和生成样本质量方面更优。

32. 题目：SDEdit：基于随机微分方程的引导式图像合成与编辑
作者：Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon
摘要：引导式图像合成使普通用户能够以最小的努力创建和编辑逼真的图像。关键挑战在于平衡对用户输入（如手绘彩色笔画）的忠实度和合成图像的真实感。现有的基于生成对抗网络（GAN）的方法试图通过条件生成对抗网络或生成对抗网络反演来实现这种平衡，这些方法具有挑战性，通常需要为各个应用提供额外的训练数据或损失函数。为解决这些问题，我们引入了一种新的图像合成和编辑方法，随机微分编辑（SDEdit），该方法基于扩散模型生成先验，通过迭代去噪随机微分方程（SDE）来合成逼真的图像。给定带有任意类型用户引导的输入图像，SDEdit首先向输入添加噪声，然后通过SDE先验对生成的图像进行去噪以提高其真实感。SDEdit不需要特定任务的训练或反演，可以自然地实现真实感和忠实度之间的平衡。根据人类感知研究，在多个任务上，包括基于笔画的图像合成与编辑以及图像合成，SDEdit在真实感上比最先进的基于GAN的方法高出98.09%，在整体满意度上高出91.72%。
