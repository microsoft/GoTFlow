---
题目：只需关注力
作者：Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
摘要：主导的序列转换模型基于复杂的循环或卷积神经网络，采用编码器-解码器配置。性能最好的模型还通过关注机制将编码器和解码器连接起来。我们提出了一种新的简单网络架构，即Transformer，它完全基于关注机制，摒弃了循环和卷积。在两个机器翻译任务的实验中，这些模型在质量上表现出优越性，同时具有更高的并行性，训练时间明显减少。我们的模型在WMT 2014英语-德语翻译任务上实现了28.4 BLEU，比现有的最佳结果（包括集成）提高了2个BLEU。在WMT 2014英语-法语翻译任务上，我们的模型在八个GPU上训练3.5天后，以41.8的BLEU分数创下了新的单模型最高水平，这只是文献中最佳模型的训练成本的一小部分。我们通过将其成功应用于英语成分解析（包括大量和有限的训练数据），证明了Transformer对其他任务具有很好的泛化能力。
---