这两篇论文的共性主题是：通过在互联网规模的数据上训练，大型语言模型能够获得通用能力。

第13篇论文《Attention Is All You Need》的作者提出了一种基于注意力机制的新型网络架构——Transformer，摒弃了循环和卷积神经网络。实验结果表明，这种模型在质量上优于其他模型，同时具有更高的并行性，训练时间大大缩短。Transformer在机器翻译任务上取得了优异的成绩，并在英语句法分析任务上表现出良好的泛化能力。

第14篇论文《Language Models are Few-Shot Learners》的作者展示了通过扩大语言模型规模，可以显著提高任务无关的少样本性能，有时甚至能与之前最先进的微调方法相媲美。作者训练了一个具有1750亿参数的GPT-3模型，是迄今为止参数最多的非稀疏语言模型。GPT-3在许多自然语言处理任务上表现出强大的性能，包括翻译、问答、填空任务等。同时，作者也发现GPT-3在某些数据集上的少样本学习仍然存在困难，以及在大型网络语料库上训练时面临的一些方法论问题。

结合这些参考论文，我们可以看出，Sora模型在训练过程中受益于大型语言模型的通用能力。Transformer架构为Sora提供了强大的基础，使其能够在视频生成任务上取得优异表现。同时，GPT-3的成功也为Sora提供了启示，即通过扩大模型规模，可以进一步提高其在各种任务上的性能。