这两篇论文的共性主题是：通过在互联网规模的数据上进行训练，大型语言模型能够获得通用能力。

第13篇论文《Attention Is All You Need》提出了一种基于注意力机制的新型网络架构——Transformer，摒弃了循环和卷积神经网络。实验结果表明，这种模型在质量上优于其他模型，同时具有更高的并行性，训练时间大大缩短。Transformer在机器翻译任务上取得了优异的成绩，并成功应用于英语句法分析任务。

第14篇论文《Language Models are Few-Shot Learners》展示了通过扩大语言模型规模，可以显著提高任务无关的少样本性能，有时甚至能与之前的最优微调方法相媲美。作者训练了一个具有1750亿参数的GPT-3模型，是迄今为止参数最多的非稀疏语言模型。GPT-3在多个自然语言处理任务上表现出色，包括翻译、问答、填空任务等。同时，论文也指出了GPT-3在某些数据集上的少样本学习仍然存在问题，以及在大型网络语料库上训练时面临的一些方法论问题。

结合这些参考论文，我们可以看出，在Sora模型的训练过程中，这些研究成果为其提供了基于注意力机制的Transformer架构以及大规模训练的优势。这使得Sora模型能够在视频生成和编辑任务上表现出色，同时具有很强的通用性和灵活性。